{
 "metadata": {
  "name": "sentiment analysis"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import enchant\n",
      "import re\n",
      "import numpy as np\n",
      "import nltk\n",
      "import string\n",
      "import yaml\n",
      "from nltk.stem.porter import PorterStemmer\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"get training dataset\"\"\"\n",
      "\n",
      "## Read in data\n",
      "location_dir='/Users/zyenge/Dropbox/climatechangeviz/'\n",
      "#location_dir='../'\n",
      "train=pd.read_csv(location_dir+'trainingandtestdata/training.1600000.processed.noemoticon.csv',header=None)\n",
      "test=pd.read_csv(location_dir+'trainingandtestdata/testdata.manual.2009.06.14.csv',header=None)\n",
      "train_df=pd.DataFrame({'tweets':train[5],'polar':train[0]})\n",
      "test_df=pd.DataFrame({'tweets':test[5],'polar':test[0]})\n",
      "\n",
      "# a subset\n",
      "train_subset=pd.concat([train_df[:500],train_df[-500:]])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"Pre-process:\n",
      "        1. remove url, @user,# -> ',', emoticon -- http://datagenetics.com/blog/october52012/index.html\n",
      "        2. if spell check is false:\n",
      "            if it's a word \"if re.match('^[a-zA-Z\\']+$',\"trainingLate\"):\"\n",
      "             try find in replace keyword dict\n",
      "             if not found: d.suggest[0]\n",
      "             elif no suggestion: shorten\n",
      "             then spell check...\n",
      "\n",
      "\n",
      "        3. tagging \n",
      "        4. stemming  \n",
      "        5. remove '-','_','(',')', etc \n",
      "        \n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"############### ---Pre-process--- #################\"\"\"\n",
      "\n",
      "\"\"\"get emoticons\"\"\"\n",
      "pos_emoticons=set([])\n",
      "neg_emoticons=set([])\n",
      "location_dir='/Users/zyenge/Dropbox/climatechangeviz/'\n",
      "#location_dir = '../'\n",
      "\n",
      "with open(location_dir+'pos_emoticon.txt') as f:\n",
      "    for line in f:\n",
      "        pos_emoticons.add(line.rstrip('\\n'))\n",
      "with open(location_dir+'neg_emoticon.txt') as f:\n",
      "    for line in f:\n",
      "        neg_emoticons.add(line.rstrip('\\n'))\n",
      "\n",
      "\"\"\"#######################################\"\"\"\n",
      "\n",
      "\"\"\"1. remove url, @user, \n",
      "    2. # -> ',' \n",
      "    3. detect and remove emoticon\"\"\"\n",
      "\n",
      "def strip_tweet(tweet):\n",
      "    no_url=re.sub('((www\\.[^\\s]+)|(http[^\\s]+))','',tweet)\n",
      "    #print no_url\n",
      "    no_user=re.sub('@[^\\s:#]+','',no_url)\n",
      "    #print no_user\n",
      "    no_hashtag=re.sub(r'#([^\\s#]+)', r'\\1,', no_user)\n",
      "    #print no_hashtag\n",
      "    return no_hashtag   \n",
      "        \n",
      "def find_emoticons(tweet):\n",
      "    c=0\n",
      "    count=np.array([0,0])\n",
      "    while c< len(tweet):\n",
      "        two_char=tweet[c:c+2]\n",
      "        three_char=tweet[c:c+3]\n",
      "        if two_char in pos_emoticons:\n",
      "            tweet=tweet[:c]+' '+tweet[c+2:]\n",
      "            count+=np.array([1,0])\n",
      "        elif three_char in pos_emoticons:\n",
      "            tweet=tweet[:c]+' '+tweet[c+3:]\n",
      "            count+=np.array([1,0])\n",
      "        elif two_char in neg_emoticons:\n",
      "            tweet=tweet[:c]+' '+tweet[c+2:]\n",
      "            count+=np.array([0,1])\n",
      "        elif three_char in neg_emoticons:\n",
      "            tweet=tweet[:c]+' '+tweet[c+3:]\n",
      "            count+=np.array([0,1])\n",
      "        c+=1\n",
      "    return (tweet,count)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"spell check and replace word\n",
      "\n",
      "if spell check is false:\n",
      "            if it's a word \"if re.match('^[a-zA-Z\\']+$',\"trainingLate\"):\"\n",
      "             try find in replace keyword dict\n",
      "             if not found: d.suggest[0]\n",
      "             elif no suggestion: shorten\n",
      "             then spell check...\n",
      "             \n",
      "\"\"\"\n",
      "\n",
      "with open(\"web_words.yaml\", 'r') as f:\n",
      "    web_dict = yaml.load(f)\n",
      "\n",
      "d=enchant.Dict('en-US')\n",
      "def shorten_word(word):\n",
      "    shortened=''\n",
      "    for c,i in enumerate(word):\n",
      "        if c<len(word)-1 and i!=word[c+1] or c==len(word)-1:\n",
      "            shortened=shortened+i\n",
      "    return shortened\n",
      "\n",
      "def replace_word(word):\n",
      "    try:\n",
      "        if word is '':\n",
      "            return word\n",
      "        elif word in web_dict:\n",
      "            return web_dict[word]\n",
      "        else:\n",
      "            \n",
      "            if d.check(word)==True:\n",
      "                return word\n",
      "            elif len(d.suggest(word))!=0: \n",
      "                return d.suggest(word)[0]\n",
      "            elif len(d.suggest(word))==0 and shorten_word(word)==word:\n",
      "                return word\n",
      "            else:\n",
      "                return replace_word(shorten_word(word))\n",
      "    except:\n",
      "        print word\n",
      "        return word\n",
      "\n",
      "\n",
      "puncset=set(string.punctuation) \n",
      "def spell_check(tweet):    \n",
      "    text_list = tweet.split()\n",
      "    for i in range(0,len(text_list)):\n",
      "        temp = ''.join([e if e not in puncset else '' for e in text_list[i]])\n",
      "        clean_word = replace_word(temp)\n",
      "        text_list[i] = text_list[i].replace(temp,clean_word)\n",
      "    return ' '.join(text_list)\n",
      "\n",
      "### Tagging\n",
      "\n",
      "def tagger(tags,text,pos_tagger,isBoolean): \n",
      "    _tags = {t:0 for t in tags}\n",
      "    processed_count = 0\n",
      "    processed_count +=1\n",
      "    text_tags = pos_tagger(nltk.word_tokenize(text))\n",
      "    #boolean approach - is pos_tag in tweet?\n",
      "    for elem in text_tags:\n",
      "        if elem[1] in _tags:\n",
      "            _tags[elem[1]] +=1\n",
      "            \n",
      "    __tags = []\n",
      "    if isBoolean:\n",
      "        for t in tags:\n",
      "            __tags.append(int(_tags[t]>0))\n",
      "    else:\n",
      "        for t in tags:\n",
      "            __tags.append(_tags[t])\n",
      "    return np.array(__tags)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def remove_punc(tweet):\n",
      "    return ''.join([e if e not in puncset else ' ' for e in tweet])\n",
      "def stemmer(tweet):\n",
      "    p = PorterStemmer()\n",
      "    text_list = tweet.split()\n",
      "    for i in range(0,len(text_list)):\n",
      "        text_list[i] = p.stem(text_list[i])   \n",
      "    return ' '.join(text_list)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "PREPROCESSING STEPS\n",
      "\"\"\"\n",
      "#input tweet\n",
      "test_tweet='@biggestfan#justin#lmao,http://somelink_to_someplace.com i <3 justin b, you\\'s r the best! xoxo'\n",
      "\n",
      "\n",
      "def pre_process_a_tweet(tweet):\n",
      "    # 0. input tweet\n",
      "    print \"original: \\n\", tweet, '\\n'\n",
      "\n",
      "    # 1. clean urls/@/#, featurize/strip emoticons\n",
      "    tweet_stripped_emoticons = find_emoticons(strip_tweet(tweet))\n",
      "    print \"no url, no #: \\n\", tweet_stripped_emoticons[0], '\\n'\n",
      "    \n",
      "    # 2. spell check tweet\n",
      "    tweet_stripped = spell_check(tweet_stripped_emoticons[0])\n",
      "    print \"spell checked: \\n\", tweet_stripped, '\\n'\n",
      "    \n",
      "    # 3. pos tagging tweet using nltk.pos_tag,isBoolean=True\n",
      "    #tags of interest\n",
      "    tweet_tagger=tagger(pos_tags,tweet_stripped,nltk.pos_tag,True)\n",
      "    print \"tagger: \\n\", tweet_tagger, '\\n'\n",
      "    \n",
      "    #4. stemming\n",
      "    tweet_stripped_tagged_stemmed = stemmer(tweet_stripped)\n",
      "    print \"stemmed: \\n\",tweet_stripped_tagged_stemmed, '\\n'\n",
      "    \n",
      "    # 4. remove punctuation\n",
      "    tweet_stripped_tagged_stemmed_nopunc = remove_punc(tweet_stripped_tagged_stemmed )\n",
      "    print \"no punctuation: \\n\",tweet_stripped_tagged_stemmed_nopunc, '\\n'\n",
      "\n",
      "    return(tweet_stripped_tagged_stemmed_nopunc, tweet_stripped_emoticons[1],tweet_tagger)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def feature_extract(tweet, transformer):\n",
      "    preprocessed_tweet=pre_process_a_tweet(test_tweet)\n",
      "    transformed_features=transformer.transform([preprocessed_tweet[0]]).toarray().flatten()\n",
      "    return np.hstack((transformed_features,preprocessed_tweet[1],preprocessed_tweet[2]))\n",
      "feature_extract('hello, lmao, xoxo',transformer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "original: \n",
        "@biggestfan#justin#lmao,http://somelink_to_someplace.com i <3 justin b, you's r the best! xoxo \n",
        "\n",
        "no url, no #: \n",
        "justin,lmao,, i   justin b, you's r the best! xoxo \n",
        "\n",
        "spell checked: \n",
        "justin,lmao,, i Justin b, you's are the best! xoxo \n",
        "\n",
        "tagger: \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0 1 0 0 1 0] \n",
        "\n",
        "stemmed: \n",
        "justin,lmao,, i Justin b, you' are the best! xoxo \n",
        "\n",
        "no punctuation: \n",
        "justin lmao   i Justin b  you  are the best  xoxo \n",
        "\n"
       ]
      },
      {
       "output_type": "pyout",
       "prompt_number": 94,
       "text": [
        "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 1, 0, 0, 1, 0, 0, 1, 0])"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pre_process_a_tweet(tweet):\n",
      "    tweet_stripped_emoticons = find_emoticons(strip_tweet(tweet))\n",
      "    tweet_stripped = spell_check(tweet_stripped_emoticons[0])\n",
      "    tweet_tagger=tagger(pos_tags,tweet_stripped,nltk.pos_tag,True)\n",
      "    tweet_stripped_tagged_stemmed = stemmer(tweet_stripped)\n",
      "    tweet_stripped_tagged_stemmed_nopunc = remove_punc(tweet_stripped_tagged_stemmed )\n",
      "    return(tweet_stripped_tagged_stemmed_nopunc, tweet_stripped_emoticons[1],tweet_tagger)\n",
      "\n",
      "\n",
      "\n",
      "#Feature selection \n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def GetStopWords():\n",
      "    stopwords_set=set(stopwords.words('english'))\n",
      "    stopwords_set.discard('not')\n",
      "    stopwords_set.discard('no')\n",
      "    stopwords_set.add('rt')\n",
      "    stopwords_list=list(stopwords_set)\n",
      "    return stopwords_list\n",
      "pos_tags = ['CC','NN','VB','JJ','NNP','RB']\n",
      "\n",
      "CV=CountVectorizer(ngram_range=(1, 3), max_features=500, stop_words=GetStopWords(),charset_error ='ignore', binary=False)\n",
      "def fit_transform(X):\n",
      "    \"\"\"X is tweet column \"\"\"\n",
      "    CV.fit(X)\n",
      "    fea = CV.transform(X)\n",
      "    feature_transformer=CV\n",
      "    if hasattr(fea, \"toarray\"):\n",
      "        extracted=fea.toarray()\n",
      "    return (extracted,feature_transformer)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "#training set data frame with two columns, 'tweets' and 'polar'[ity]\n",
      "#also concatenates polarity as of now\n",
      "def build_feature_matrix(train_subset):\n",
      "    #build feature vector\n",
      "    emoticon_feature=[]\n",
      "    pos_tag_feature=[]\n",
      "    tweets_list=[]\n",
      "    for i in train_subset['tweets']:\n",
      "        processed_tweet=pre_process_a_tweet(i)\n",
      "        tweets_list.append(processed_tweet[0])\n",
      "        #emoticon features\n",
      "        emoticon_feature.append(processed_tweet[1])\n",
      "        #POS features\n",
      "        pos_tag_feature.append(processed_tweet[2])\n",
      "        \n",
      "    print len(tweets_list)\n",
      "    \n",
      "    text_features, transformer=fit_transform(np.array(tweets_list))\n",
      "    \n",
      "    text_features=np.hstack((text_features,pd.DataFrame(emoticon_feature),pd.DataFrame(pos_tag_feature)))\n",
      "    return text_features,transformer\n",
      "\n",
      "feature_matrix,transformer = build_feature_matrix(train_subset)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1000\n"
       ]
      }
     ],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_features,test_features,train_labels,test_labels = train_test_split(feature_matrix,pd.DataFrame(train_subset['polar']),test_size=0.20)\n",
      "train_labels = np.ravel(train_labels)\n",
      "test_labels = np.ravel(test_labels)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 112
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Classifiers Below\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 113,
       "text": [
        "'\\nClassifiers Below\\n'"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "rfc_clf = RandomForestClassifier(n_estimators=100,verbose=0,n_jobs=1,random_state=None)\n",
      "rfc_clf.fit(train_features,train_labels)\n",
      "rfc_clf.score(test_features,test_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 114,
       "text": [
        "0.68500000000000005"
       ]
      }
     ],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "mnb_clf = MultinomialNB(alpha=1.0,fit_prior=True,class_prior=None)\n",
      "mnb_clf.fit(train_features,train_labels)\n",
      "mnb_clf.score(test_features,test_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 115,
       "text": [
        "0.71499999999999997"
       ]
      }
     ],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "lr_clf = LogisticRegression()\n",
      "lr_clf.fit(train_features,train_labels)\n",
      "lr_clf.score(test_features,test_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 116,
       "text": [
        "0.69999999999999996"
       ]
      }
     ],
     "prompt_number": 116
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rfc_clf.predict_proba(feature_extract(\"I hate\", transformer))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 128,
       "text": [
        "array([[ 0.41,  0.59]])"
       ]
      }
     ],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "#if kernel = 'rbf', score is quite low\n",
      "svm_clf = SVC(kernel='linear')\n",
      "svm_clf.fit(train_features,train_labels)\n",
      "svm_clf.score(test_features,test_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 295,
       "text": [
        "0.66500000000000004"
       ]
      }
     ],
     "prompt_number": 295
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}