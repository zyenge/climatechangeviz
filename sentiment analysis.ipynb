{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import enchant\n",
      "import re\n",
      "import numpy as np\n",
      "\"\"\"\n",
      "pos=['funny movie, i enjoyed it','luv it',\"highly recommend, wouldn't miss it for the world\",\"what an awesome movie\"]\n",
      "neg=['it sucks','boring, waste of time', 'i hate it,i cannot think of anyone who would like it',\"it's terriable, it sucks so much, i want my money and time back\"]\n",
      "\n",
      "pos_df=pd.DataFrame(pos, columns=[\"review\"])\n",
      "pos_df['polar']=1\n",
      "neg_df=pd.DataFrame(neg, columns=[\"review\"])\n",
      "neg_df['polar']=-1\n",
      "\n",
      "df=pos_df.append(neg_df,ignore_index=True) \n",
      "#--------------------\n",
      "\"\"\"\n",
      "#200+tag+emoticon(1+1)+len(sentence)\n",
      "## Read in data\n",
      "#location_dir='/Users/zyenge/Dropbox/climatechangeviz/'\n",
      "location_dir='../'\n",
      "train=pd.read_csv(location_dir+'trainingandtestdata/training.1600000.processed.noemoticon.csv',header=None)\n",
      "test=pd.read_csv(location_dir+'trainingandtestdata/testdata.manual.2009.06.14.csv',header=None)\n",
      "train_df=pd.DataFrame({'tweets':train[5],'polar':train[0]})\n",
      "test_df=pd.DataFrame({'tweets':test[5],'polar':test[0]})\n",
      "\n",
      "# a subset\n",
      "train_subset=pd.concat([train_df[:10000],train_df[-10000:]])\n",
      "#print train_subset\n",
      "\n",
      "\"\"\"cleanning sequence:\n",
      "        0. remove url, @user,# -> ',', emoticon -- http://datagenetics.com/blog/october52012/index.html\n",
      "        2. if spell check is false:\n",
      "            if it's a word \"if re.match('^[a-zA-Z\\']+$',\"trainingLate\"):\"\n",
      "             try find in replace keyword dict\n",
      "             if not found: d.suggest[0]\n",
      "             elif no suggestion: shorten\n",
      "             then spell check...\n",
      "\n",
      "\n",
      "        3. tagging \n",
      "        4. remove '-','_','(',')', etc \n",
      "        5. stemming\n",
      "        #5. replace keywords (not enough training)\n",
      "\n",
      "    \"\"\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 297,
       "text": [
        "'cleanning sequence:\\n        0. remove url, @user,# -> \\',\\', emoticon -- http://datagenetics.com/blog/october52012/index.html\\n        2. if spell check is false:\\n            if it\\'s a word \"if re.match(\\'^[a-zA-Z\\']+$\\',\"trainingLate\"):\"\\n             try find in replace keyword dict\\n             if not found: d.suggest[0]\\n             elif no suggestion: shorten\\n             then spell check...\\n\\n\\n        3. tagging \\n        4. remove \\'-\\',\\'_\\',\\'(\\',\\')\\', etc \\n        5. stemming\\n        #5. replace keywords (not enough training)\\n\\n    '"
       ]
      }
     ],
     "prompt_number": 297
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "#pos_string='i <3 justin b, u r the best! xoxo'\n",
      "#neg_string='it\\'s said that young people are blindly following jb..:('\n",
      "\n",
      "\n",
      "pos_emoticons=set([])\n",
      "neg_emoticons=set([])\n",
      "#location_dir='/Users/zyenge/Dropbox/climatechangeviz/'\n",
      "location_dir = ''\n",
      "#smiley:\n",
      "with open(location_dir+'pos_emoticon.txt') as f:\n",
      "    for line in f:\n",
      "        pos_emoticons.add(line.rstrip('\\n'))\n",
      "#frowney:\n",
      "with open(location_dir+'neg_emoticon.txt') as f:\n",
      "    for line in f:\n",
      "        neg_emoticons.add(line.rstrip('\\n'))\n",
      "\n",
      "\"\"\"1. remove url, @user, \n",
      "    2. # -> ',' \n",
      "    3. detect and remove emoticon\"\"\"\n",
      "\n",
      "def strip_tweet(tweet):\n",
      "    no_url=re.sub('((www\\.[^\\s]+)|(http[^\\s]+))','',tweet)\n",
      "    #print no_url\n",
      "    no_user=re.sub('@[^\\s]+','',no_url)\n",
      "    #print no_user\n",
      "    no_hashtag=re.sub(r'#([^\\s#]+)', r'\\1,', no_user)\n",
      "    #print no_hashtag\n",
      "    return no_hashtag   \n",
      "        \n",
      "def find_emoticons(tweet):\n",
      "    c=0\n",
      "    count=np.array([0,0])\n",
      "    while c< len(tweet):\n",
      "        two_char=tweet[c:c+2]\n",
      "        three_char=tweet[c:c+3]\n",
      "        if two_char in pos_emoticons:\n",
      "            tweet=tweet[:c]+' '+tweet[c+2:]\n",
      "            count+=np.array([1,0])\n",
      "        elif three_char in pos_emoticons:\n",
      "            tweet=tweet[:c]+' '+tweet[c+3:]\n",
      "            count+=np.array([1,0])\n",
      "        elif two_char in neg_emoticons:\n",
      "            tweet=tweet[:c]+' '+tweet[c+2:]\n",
      "            count+=np.array([0,1])\n",
      "        elif three_char in neg_emoticons:\n",
      "            tweet=tweet[:c]+' '+tweet[c+3:]\n",
      "            count+=np.array([0,1])\n",
      "        c+=1\n",
      "    return (tweet,count)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 298
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"spell check and replace word\n",
      "\n",
      "if spell check is false:\n",
      "            if it's a word \"if re.match('^[a-zA-Z\\']+$',\"trainingLate\"):\"\n",
      "             try find in replace keyword dict\n",
      "             if not found: d.suggest[0]\n",
      "             elif no suggestion: shorten\n",
      "             then spell check...\n",
      "             \n",
      "\"\"\"\n",
      "import enchant\n",
      "import string\n",
      "import yaml\n",
      "with open(\"web_words.yaml\", 'r') as f:\n",
      "    web_dict = yaml.load(f)\n",
      "\n",
      "puncset=set(string.punctuation) \n",
      "\n",
      "d=enchant.Dict('en-US')\n",
      "\n",
      "def shorten_word(word):\n",
      "    shortened=''\n",
      "    for c,i in enumerate(word):\n",
      "        if c<len(word)-1 and i!=word[c+1] or c==len(word)-1:\n",
      "            shortened=shortened+i\n",
      "    return shortened\n",
      "shorten_word('vvveeeeelll')\n",
      "\n",
      "def replace_word(word):\n",
      "    try:\n",
      "        if word is '':\n",
      "            return word\n",
      "        elif word in web_dict:\n",
      "            return web_dict[word]\n",
      "        else:\n",
      "            \n",
      "            if d.check(word)==True:\n",
      "                return word\n",
      "            elif len(d.suggest(word))!=0: \n",
      "                return d.suggest(word)[0]\n",
      "            elif len(d.suggest(word))==0 and shorten_word(word)==word:\n",
      "                return word\n",
      "            else:\n",
      "                return replace_word(shorten_word(word))\n",
      "    except:\n",
      "        print word\n",
      "        return word\n",
      "\n",
      "def spell_check(tweet):    \n",
      "    text_list = tweet.split()\n",
      "    for i in range(0,len(text_list)):\n",
      "        temp = ''.join([e if e not in puncset else '' for e in text_list[i]])\n",
      "        clean_word = replace_word(temp)\n",
      "        text_list[i] = text_list[i].replace(temp,clean_word)\n",
      "    return ' '.join(text_list)\n",
      "\n",
      "### Tagging\n",
      "import pandas as pd\n",
      "import nltk\n",
      "def tagger(tags,text,pos_tagger,isBoolean): \n",
      "    _tags = {t:0 for t in tags}\n",
      "    processed_count = 0\n",
      "    processed_count +=1\n",
      "    text_tags = pos_tagger(nltk.word_tokenize(text))\n",
      "    #boolean approach - is pos_tag in tweet?\n",
      "    for elem in text_tags:\n",
      "        if elem[1] in _tags:\n",
      "            _tags[elem[1]] +=1\n",
      "            \n",
      "    __tags = []\n",
      "    if isBoolean:\n",
      "        for t in tags:\n",
      "            __tags.append(int(_tags[t]>0))\n",
      "    else:\n",
      "        for t in tags:\n",
      "            __tags.append(_tags[t])\n",
      "    return np.array(__tags)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 309
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem.porter import PorterStemmer\n",
      "\n",
      "def remove_punc(tweet):\n",
      "    return ''.join([e if e not in puncset else '' for e in tweet])\n",
      "def stemmer(tweet):\n",
      "    p = PorterStemmer()\n",
      "    text_list = tweet.split()\n",
      "    for i in range(0,len(text_list)):\n",
      "        text_list[i] = p.stem(text_list[i])   \n",
      "    return ' '.join(text_list)\n",
      "    \n",
      "                \n",
      "remove_punc(\"he!!o :)\")\n",
      "stemmer('')\n",
      "# stemming"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 310,
       "text": [
        "''"
       ]
      }
     ],
     "prompt_number": 310
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "PREPROCESSING STEPS\n",
      "\"\"\"\n",
      "\n",
      "#input tweet\n",
      "test_tweet='@biggestfan#justin#lmao,http://somelink_to_someplace.com i <3 justin b, you\\'s r the best! xoxo'\n",
      "print test_tweet\n",
      "\n",
      "#clean urls/@/#, featurize/strip emoticons\n",
      "test_tweet_stripped = find_emoticons(strip_tweet(test_tweet))\n",
      "print test_tweet_stripped\n",
      "\n",
      "#spell check tweet\n",
      "test_tweet_stripped = (spell_check(test_tweet_stripped[0]),test_tweet_stripped[1])\n",
      "print test_tweet_stripped\n",
      "\n",
      "#pos tagging tweet using nltk.pos_tag,isBoolean=True\n",
      "#tags of interest\n",
      "pos_tags = ['CC','NN','VB','JJ','NNP','RB']\n",
      "test_tweet_stripped_tagged = (test_tweet_stripped[0],test_tweet_stripped[1],tagger(pos_tags,test_tweet_stripped[0],nltk.pos_tag,True))\n",
      "print test_tweet_stripped_tagged\n",
      "\n",
      "#remove punctuation\n",
      "test_tweet_stripped_tagged_nopunc = (remove_punc(test_tweet_stripped_tagged[0]),test_tweet_stripped_tagged[1],test_tweet_stripped_tagged[2])\n",
      "print test_tweet_stripped_tagged_nopunc\n",
      "\n",
      "#stemming\n",
      "test_tweet_stripped_tagged_nopunc_stemmed = (stemmer(test_tweet_stripped_tagged_nopunc[0]),test_tweet_stripped_tagged_nopunc[1],test_tweet_stripped_tagged_nopunc[2])\n",
      "print test_tweet_stripped_tagged_nopunc_stemmed"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "@biggestfan#justin#lmao,http://somelink_to_someplace.com i <3 justin b, you's r the best! xoxo\n",
        "(\" i   justin b, you's r the best! xoxo\", array([1, 0]))\n",
        "(\"i Justin b, you's are the best! xoxo\", array([1, 0]))\n",
        "(\"i Justin b, you's are the best! xoxo\", array([1, 0]), array([0, 1, 0, 0, 1, 0]))\n",
        "('i Justin b yous are the best xoxo', array([1, 0]), array([0, 1, 0, 0, 1, 0]))\n",
        "('i Justin b you are the best xoxo', array([1, 0]), array([0, 1, 0, 0, 1, 0]))\n"
       ]
      }
     ],
     "prompt_number": 311
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Feature selection \n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from nltk.corpus import stopwords\n",
      "def GetStopWords():\n",
      "    stopwords_set=set(stopwords.words('english'))\n",
      "    stopwords_set.discard('not')\n",
      "    stopwords_set.discard('no')\n",
      "    #stopwordstring=yaml.load(open(\"stopwords.yaml\"))\n",
      "    stopwordstring='movie, review'\n",
      "    stopwordlist=stopwordstring.split(',')\n",
      "    for word in stopwordlist:\n",
      "        stopwords_set.add(word.strip())\n",
      "    stopwords_list=list(stopwords_set)\n",
      "    return stopwords_list\n",
      "\n",
      "#features = [('review', CountVectorizer(min_df=2, ngram_range=(1, 3), decode_error= 'replace',max_features=300, stop_words=GetStopWords(),binary=False))\n",
      "#             ]\n",
      "\n",
      "\n",
      "#CV=CountVectorizer(ngram_range=(1, 3), max_features=300,  stop_words='english',binary=False)\n",
      "CV=CountVectorizer(ngram_range=(1, 3), max_features=300,  stop_words=GetStopWords(),binary=False)\n",
      "def fit_transform(X):\n",
      "    \"\"\"X is tweet column \"\"\"\n",
      "    CV.fit(X)\n",
      "    fea = CV.transform(X)\n",
      "    feature_transformer=CV\n",
      "    if hasattr(fea, \"toarray\"):\n",
      "        extracted=fea.toarray()\n",
      "    return (extracted,feature_transformer)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 312
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "#training set data frame with two columns, 'tweets' and 'polar'[ity]\n",
      "#also concatenates polarity as of now\n",
      "def build_feature_matrix(train_subset):\n",
      "    #build feature vector\n",
      "    emoticon_feature=[]\n",
      "    pos_tag_feature=[]\n",
      "    tweets_list=[]\n",
      "    for i in train_subset['tweets']:\n",
      "        #emoticon features\n",
      "        tweet_pair=find_emoticons(strip_tweet(i))\n",
      "        emoticon_feature.append(tweet_pair[1])\n",
      "        #POS features\n",
      "        pos_tag_feature.append(tagger(pos_tags,spell_check(tweet_pair[0]),nltk.pos_tag,True))\n",
      "        tweets_list.append(stemmer(remove_punc(tweet_pair[0])))\n",
      "        #tweets_list.append(i)\n",
      "    \n",
      "    print len(tweets_list)\n",
      "    \n",
      "    \n",
      "    #transformer=fit_transform(train_subset['tweets'])[1]\n",
      "    \n",
      "    #text_features=fit_transform(train_subset['tweets'])[0]\n",
      "    text_features=fit_transform(tweets_list)[0]\n",
      "    text_features=np.hstack((text_features,pd.DataFrame(emoticon_feature),pd.DataFrame(pos_tag_feature)))\n",
      "    return text_features\n",
      "\n",
      "feature_matrix = build_feature_matrix(train_subset)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_features,test_features,train_labels,test_labels = train_test_split(feature_matrix,pd.DataFrame(train_subset['polar']),test_size=0.20)\n",
      "train_labels = np.ravel(train_labels)\n",
      "test_labels = np.ravel(test_labels)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 287
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Classifiers Below\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "rfc_clf = RandomForestClassifier(n_estimators=20,verbose=0,n_jobs=1,random_state=None)\n",
      "rfc_clf.fit(train_features,train_labels)\n",
      "rfc_clf.score(test_features,test_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "mnb_clf = MultinomialNB(alpha=1.0,fit_prior=True,class_prior=None)\n",
      "mnb_clf.fit(train_features,train_labels)\n",
      "mnb_clf.score(test_features,test_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 293,
       "text": [
        "0.69999999999999996"
       ]
      }
     ],
     "prompt_number": 293
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "lr_clf = LogisticRegression()\n",
      "lr_clf.fit(train_features,train_labels)\n",
      "lr_clf.score(test_features,test_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 294,
       "text": [
        "0.72499999999999998"
       ]
      }
     ],
     "prompt_number": 294
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "#if kernel = 'rbf', score is quite low\n",
      "svm_clf = SVC(kernel='linear')\n",
      "svm_clf.fit(train_features,train_labels)\n",
      "svm_clf.score(test_features,test_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 295,
       "text": [
        "0.66500000000000004"
       ]
      }
     ],
     "prompt_number": 295
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}