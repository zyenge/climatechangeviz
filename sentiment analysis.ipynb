{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import enchant\n",
      "import re\n",
      "import numpy as np\n",
      "\"\"\"\n",
      "pos=['funny movie, i enjoyed it','luv it',\"highly recommend, wouldn't miss it for the world\",\"what an awesome movie\"]\n",
      "neg=['it sucks','boring, waste of time', 'i hate it,i cannot think of anyone who would like it',\"it's terriable, it sucks so much, i want my money and time back\"]\n",
      "\n",
      "pos_df=pd.DataFrame(pos, columns=[\"review\"])\n",
      "pos_df['polar']=1\n",
      "neg_df=pd.DataFrame(neg, columns=[\"review\"])\n",
      "neg_df['polar']=-1\n",
      "\n",
      "df=pos_df.append(neg_df,ignore_index=True) \n",
      "#--------------------\n",
      "\"\"\"\n",
      "#200+tag+emoticon(1+1)+len(sentence)\n",
      "## Read in data\n",
      "#location_dir='/Users/zyenge/Dropbox/climatechangeviz/'\n",
      "location_dir='../'\n",
      "train=pd.read_csv(location_dir+'trainingandtestdata/training.1600000.processed.noemoticon.csv',header=None)\n",
      "test=pd.read_csv(location_dir+'trainingandtestdata/testdata.manual.2009.06.14.csv',header=None)\n",
      "train_df=pd.DataFrame({'tweets':train[5],'polar':train[0]})\n",
      "test_df=pd.DataFrame({'tweets':test[5],'polar':test[0]})\n",
      "\n",
      "# a subset\n",
      "train_subset=pd.concat([train_df[:50],train_df[-50:]])\n",
      "train_subset\n",
      "\n",
      "\n",
      "#d.check('li')\n",
      "#d.suggest('booked')\n",
      "\n",
      "\n",
      "\n",
      "\"\"\"cleanning sequence:\n",
      "        0. remove url, @user,# -> ',', emoticon -- http://datagenetics.com/blog/october52012/index.html\n",
      "        2. if spell check is false:\n",
      "            if it's a word \"if re.match('^[a-zA-Z\\']+$',\"trainingLate\"):\"\n",
      "             try find in replace keyword dict\n",
      "             if not found: d.suggest[0]\n",
      "             elif no suggestion: shorten\n",
      "             then spell check...\n",
      "\n",
      "\n",
      "        3. tagging \n",
      "        4. remove '-','_','(',')', etc \n",
      "        5. stemming\n",
      "        #5. replace keywords (not enough training)\n",
      "\n",
      "    \"\"\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "'cleanning sequence:\\n        0. remove url, @user,# -> \\',\\', emoticon -- http://datagenetics.com/blog/october52012/index.html\\n        2. if spell check is false:\\n            if it\\'s a word \"if re.match(\\'^[a-zA-Z\\']+$\\',\"trainingLate\"):\"\\n             try find in replace keyword dict\\n             if not found: d.suggest[0]\\n             elif no suggestion: shorten\\n             then spell check...\\n\\n\\n        3. tagging \\n        4. remove \\'-\\',\\'_\\',\\'(\\',\\')\\', etc \\n        5. stemming\\n        #5. replace keywords (not enough training)\\n\\n    '"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "#pos_string='i <3 justin b, u r the best! xoxo'\n",
      "#neg_string='it\\'s said that young people are blindly following jb..:('\n",
      "\n",
      "\n",
      "pos_emoticons=set([])\n",
      "neg_emoticons=set([])\n",
      "#location_dir='/Users/zyenge/Dropbox/climatechangeviz/'\n",
      "location_dir = ''\n",
      "#smiley:\n",
      "with open(location_dir+'pos_emoticon.txt') as f:\n",
      "    for line in f:\n",
      "        pos_emoticons.add(line.rstrip('\\n'))\n",
      "#frowney:\n",
      "with open(location_dir+'neg_emoticon.txt') as f:\n",
      "    for line in f:\n",
      "        neg_emoticons.add(line.rstrip('\\n'))\n",
      "\n",
      "        \n",
      "\n",
      "\"\"\"1. remove url, @user, \n",
      "    2. # -> ',' \n",
      "    3. detect and remove emoticon\"\"\"\n",
      "\n",
      "def strip_tweet(tweet):\n",
      "    no_url=re.sub('((www\\.[^\\s]+)|(http[^\\s]+))','',tweet)\n",
      "    #print no_url\n",
      "    no_user=re.sub('@[^\\s]+','',no_url)\n",
      "    #print no_user\n",
      "    no_hashtag=re.sub(r'#([^\\s#]+)', r'\\1,', no_user)\n",
      "    #print no_hashtag\n",
      "    return no_hashtag   \n",
      "        \n",
      "def find_emoticons(tweet):\n",
      "    c=0\n",
      "    count=np.array([0,0])\n",
      "    while c< len(tweet):\n",
      "        two_char=tweet[c:c+2]\n",
      "        three_char=tweet[c:c+3]\n",
      "        if two_char in pos_emoticons:\n",
      "            tweet=tweet[:c]+' '+tweet[c+2:]\n",
      "            count+=np.array([1,0])\n",
      "        elif three_char in pos_emoticons:\n",
      "            tweet=tweet[:c]+' '+tweet[c+3:]\n",
      "            count+=np.array([1,0])\n",
      "        elif two_char in neg_emoticons:\n",
      "            tweet=tweet[:c]+' '+tweet[c+2:]\n",
      "            count+=np.array([0,1])\n",
      "        elif three_char in neg_emoticons:\n",
      "            tweet=tweet[:c]+' '+tweet[c+3:]\n",
      "            count+=np.array([0,1])\n",
      "        c+=1\n",
      "    return (tweet,count)\n",
      "\n",
      "\n",
      "emoticon_feature=[]\n",
      "pos_tag_feature=[]\n",
      "for i in train_subset['tweets']:\n",
      "    #emoticon features\n",
      "    tweet_pair=find_emoticons(strip_tweet(i))\n",
      "    emoticon_feature.append(tweet_pair[1])\n",
      "    #POS features\n",
      "    pos_tag_feature.append(tagger(pos_tags,spell_check(tweet_pair[0]),nltk.pos_tag,True))\n",
      "\n",
      "#emoticon_features=pd.DataFrame(emoticon_feature)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[array([0, 0, 1, 0, 1, 0]), array([1, 1, 1, 0, 1, 1]), array([0, 1, 1, 1, 1, 0]), array([1, 1, 0, 1, 0, 0]), array([0, 1, 1, 0, 1, 1]), array([0, 1, 0, 1, 0, 1]), array([0, 1, 0, 0, 1, 0]), array([0, 1, 0, 0, 1, 1]), array([0, 1, 1, 0, 0, 1]), array([0, 1, 0, 0, 1, 0])]\n"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"spell check and replace word\n",
      "\n",
      "if spell check is false:\n",
      "            if it's a word \"if re.match('^[a-zA-Z\\']+$',\"trainingLate\"):\"\n",
      "             try find in replace keyword dict\n",
      "             if not found: d.suggest[0]\n",
      "             elif no suggestion: shorten\n",
      "             then spell check...\n",
      "             \n",
      "\"\"\"\n",
      "import enchant\n",
      "import string\n",
      "import yaml\n",
      "with open(\"web_words.yaml\", 'r') as f:\n",
      "    web_dict = yaml.load(f)\n",
      "\n",
      "puncset=set(string.punctuation) \n",
      "\n",
      "d=enchant.Dict('en-US')\n",
      "\n",
      "def shorten_word(word):\n",
      "    shortened=''\n",
      "    for c,i in enumerate(word):\n",
      "        if c<len(word)-1 and i!=word[c+1] or c==len(word)-1:\n",
      "            shortened=shortened+i\n",
      "    return shortened\n",
      "shorten_word('vvveeeeelll')\n",
      "\n",
      "def replace_word(word):\n",
      "    if word is '':\n",
      "        return word\n",
      "    elif word in web_dict:\n",
      "        return web_dict[word]\n",
      "    else:\n",
      "        if d.check(word)==True:\n",
      "            return word\n",
      "        elif len(d.suggest(word))!=0: \n",
      "            return d.suggest(word)[0]\n",
      "        elif len(d.suggest(word))==0 and shorten_word(word)==word:\n",
      "            return word\n",
      "        else:\n",
      "            return replace_word(shorten_word(word))\n",
      "\n",
      "def spell_check(tweet):    \n",
      "    text_list = tweet.split()\n",
      "    for i in range(0,len(text_list)):\n",
      "        temp = ''.join([e if e not in puncset else '' for e in text_list[i]])\n",
      "        clean_word = replace_word(temp)\n",
      "        text_list[i] = text_list[i].replace(temp,clean_word)\n",
      "    return ' '.join(text_list)\n",
      "\n",
      "### Tagging\n",
      "import pandas as pd\n",
      "import nltk\n",
      "def tagger(tags,text,pos_tagger,isBoolean): \n",
      "    _tags = {t:0 for t in tags}\n",
      "    processed_count = 0\n",
      "    processed_count +=1\n",
      "    text_tags = pos_tagger(nltk.word_tokenize(text))\n",
      "    #boolean approach - is pos_tag in tweet?\n",
      "    for elem in text_tags:\n",
      "        if elem[1] in _tags:\n",
      "            _tags[elem[1]] +=1\n",
      "            \n",
      "    __tags = []\n",
      "    if isBoolean:\n",
      "        for t in tags:\n",
      "            __tags.append(int(_tags[t]>0))\n",
      "    else:\n",
      "        for t in tags:\n",
      "            __tags.append(_tags[t])\n",
      "    return np.array(__tags)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "PREPROCESSING STEPS\n",
      "\"\"\"\n",
      "\n",
      "#input tweet\n",
      "test_tweet='@biggestfan#justin#lmao,http://somelink_to_someplace.com i <3 justin b, u r the best! xoxo'\n",
      "print test_tweet\n",
      "\n",
      "#clean urls/@/#, featurize/strip emoticons\n",
      "test_tweet_stripped = find_emoticons(strip_tweet(test_tweet))\n",
      "print test_tweet_stripped\n",
      "\n",
      "#spell check tweet\n",
      "test_tweet_stripped = (spell_check(test_tweet_stripped[0]),test_tweet_stripped[1])\n",
      "print test_tweet_stripped\n",
      "\n",
      "#pos tagging tweet using nltk.pos_tag,isBoolean=True\n",
      "#tags of interest\n",
      "pos_tags = ['CC','NN','VB','JJ','NNP','RB']\n",
      "test_tweet_stripped_tagged = (test_tweet_stripped[0],test_tweet_stripped[1],tagger(pos_tags,test_tweet_stripped[0],nltk.pos_tag,True))\n",
      "print test_tweet_stripped_tagged"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "@biggestfan#justin#lmao,http://somelink_to_someplace.com i <3 justin b, u r the best! xoxo\n",
        "(' i   justin b, u r the best! xoxo', array([1, 0]))\n",
        "('i Justin b, you are the best! xoxo', array([1, 0]))\n",
        "('i Justin b, you are the best! xoxo', array([1, 0]), array([0, 1, 0, 0, 1, 0]))\n"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# remove punct, to_lower\n",
      "# stemming"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Feature selection \n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from nltk.corpus import stopwords\n",
      "def GetStopWords():\n",
      "    stopwords_set=set(stopwords.words('english'))\n",
      "    stopwords_set.discard('not')\n",
      "    stopwords_set.discard('no')\n",
      "    #stopwordstring=yaml.load(open(\"stopwords.yaml\"))\n",
      "    stopwordstring='movie, review'\n",
      "    stopwordlist=stopwordstring.split(',')\n",
      "    for word in stopwordlist:\n",
      "        stopwords_set.add(word.strip())\n",
      "    stopwords_list=list(stopwords_set)\n",
      "    return stopwords_list\n",
      "\n",
      "#features = [('review', CountVectorizer(min_df=2, ngram_range=(1, 3), decode_error= 'replace',max_features=300, stop_words=GetStopWords(),binary=False))\n",
      "#             ]\n",
      "\n",
      "\n",
      "#CV=CountVectorizer(ngram_range=(1, 3), max_features=300,  stop_words='english',binary=False)\n",
      "CV=CountVectorizer(ngram_range=(1, 3), max_features=300,  stop_words=GetStopWords(),binary=False)\n",
      "def fit_transform(X):\n",
      "    \"\"\"X is tweet column \"\"\"\n",
      "    CV.fit(X)\n",
      "    fea = CV.transform(X)\n",
      "    feature_transformer=CV\n",
      "    if hasattr(fea, \"toarray\"):\n",
      "        extracted=fea.toarray()\n",
      "    return (extracted,feature_transformer)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text_features=fit_transform(train_subset['tweets'])[0]\n",
      "transformer=fit_transform(train_subset['tweets'])[1]\n",
      "text_features=np.hstack((text_features,pd.DataFrame(emoticon_feature),pd.DataFrame(pos_tag_feature)))\n",
      "\n",
      "text_features.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 88,
       "text": [
        "(100, 308)"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "rfc_clf = RandomForestClassifier(n_estimators=20,verbose=0,n_jobs=1,random_state=None)\n",
      "rfc_clf.fit(text_features,train_subset['polar'])\n",
      "rfc_clf.score(text_features,train_subset['polar'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 93,
       "text": [
        "0.96999999999999997"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.\n",
      "rfc_clf = RandomForestClassifier(n_estimators=20,verbose=0,n_jobs=1,random_state=None)\n",
      "rfc_clf.fit(text_features,train_subset['polar'])\n",
      "rfc_clf.score(text_features,train_subset['polar'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 90,
       "text": [
        "array([4])"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.score(text_features,train_subset['polar'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 91,
       "text": [
        "0.97999999999999998"
       ]
      }
     ],
     "prompt_number": 91
    }
   ],
   "metadata": {}
  }
 ]
}